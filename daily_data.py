{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ak0Ru8W09y1o",
    "outputId": "0d85e07f-c169-4d65-de5f-f847721c7c0e"
   },
   "outputs": [],
   "source": [
    "# Google Drive ile bağlantı kurmak için gerekli kütüphane\n",
    "# !pip install google\n",
    "# from google.colab import drive\n",
    "import pandas as pd\n",
    "\n",
    "# # Google Drive'ı bağlama\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "def get_stock_price_history(stock_code):\n",
    "  stock_file_path = f'Data/{stock_code}.csv'\n",
    "  selected_columns = ['Date', 'Price', 'Open', 'High', 'Low', 'Change %']\n",
    "  df = pd.read_csv(stock_file_path, usecols=selected_columns)\n",
    "  df = df.rename(columns={'Price': 'Close'}) #price to close\n",
    "  df = df.rename(columns={'Change %': 'Change%'}) #price to close\n",
    "  df['Date'] = pd.to_datetime(df['Date'])\n",
    "  df_reversed = df.iloc[::-1].reset_index(drop=True) #reverse\n",
    "  df_reversed = convert_to_float(df_reversed) #string to float\n",
    "  return df_reversed\n",
    "\n",
    "# calculate rsi then add\n",
    "def calculate_rsi(df, window=14):\n",
    "    #RSI formulation\n",
    "    delta = df['Close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "\n",
    "    # add RSI\n",
    "    df['RSI'] = rsi\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# calculate ATR value\n",
    "def calculate_atr(df, window=14):\n",
    "    # True Range hesaplama\n",
    "    df['High-Low'] = df['High'] - df['Low']\n",
    "    df['High-PrevClose'] = abs(df['High'] - df['Close'].shift(1))\n",
    "    df['PrevClose-Low'] = abs(df['Close'].shift(1) - df['Low'])\n",
    "    df['TrueRange'] = df[['High-Low', 'High-PrevClose', 'PrevClose-Low']].max(axis=1)\n",
    "\n",
    "    # ATR hesaplama\n",
    "    df['ATR'] = df['TrueRange'].rolling(window=window).mean()\n",
    "\n",
    "    # Gereksiz sütunları sil\n",
    "    df.drop(['High-Low', 'High-PrevClose', 'PrevClose-Low', 'TrueRange'], axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def delete_high_low_columns(df):\n",
    "    if 'High' in df.columns and 'Low' in df.columns:\n",
    "        df.drop(['High', 'Low'], axis=1, inplace=True)\n",
    "        return df\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "\n",
    "def delete_nan_values(df):\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "# Belirli bir sütundaki NaN değerlerini siler\n",
    "def delete_nan_values_with_column(df, column):\n",
    "    df.dropna(subset=[column], inplace=True)\n",
    "    return df\n",
    "\n",
    "def convert_to_float(df):\n",
    "    columns_to_convert = ['Open', 'Close', 'Low', 'High', 'Change%']\n",
    "\n",
    "    for column in columns_to_convert:\n",
    "        try:\n",
    "          df[column] = df[column].str.replace(',', '')  # Virgülleri kaldır\n",
    "        except AttributeError:\n",
    "          continue\n",
    "        df[column] = df[column].str.replace('%', '')  # Yüzde işaretlerini kaldır\n",
    "        df[column] = df[column].astype(float)  # Float olarak dönüştür ve tamsayıya dönüştür\n",
    "\n",
    "    return df\n",
    "\n",
    "def merge_stock_and_index_by_date(df_stock, df_index):\n",
    "  # Tarih sütununu indeks olarak ayarla\n",
    "    df_stock.set_index('Date', inplace=True)\n",
    "    df_index.set_index('Date', inplace=True)\n",
    "\n",
    "    # Hisse senedi ve endeks verilerini tarihe göre birleştir\n",
    "    merged_df = df_stock.join(df_index, lsuffix='_stock', rsuffix='_index')\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def calculte_relative_price_change(merged_df, threshold=0.1):\n",
    "    try:\n",
    "        # Fiyat değişim yüzdesine göre durumu belirle\n",
    "        price_change_difference = merged_df['Change%_stock'] - merged_df['Change%_index']\n",
    "        merged_df['Relative_change'] = None  # Önce Relative_change sütununu None değeriyle başlatın\n",
    "        merged_df.loc[price_change_difference > threshold, 'Relative_change'] = \"increased\"\n",
    "        merged_df.loc[price_change_difference < -threshold, 'Relative_change'] = \"decreased\"\n",
    "        # merged_df.loc[(price_change_difference >= -threshold) & (price_change_difference <= threshold), 'Relative_change'] = \"notr\"\n",
    "\n",
    "        return merged_df\n",
    "    except Exception as e:\n",
    "        return f\"Hata: {str(e)}\"\n",
    "\n",
    "def merge_and_calculate_price_change(stock_df, index_df, threshold=0.1):\n",
    "  index_df = index_df.drop(['Open', 'Close'], axis=1, errors='ignore')\n",
    "  merged_df = merge_stock_and_index_by_date(stock_df, index_df)\n",
    "  calculated_merged_df = calculte_relative_price_change(merged_df, threshold)\n",
    "\n",
    "  return calculated_merged_df\n",
    "\n",
    "\n",
    "def shift_columns_up(df, columns):\n",
    "    for column in columns:\n",
    "        df[column] = df[column].shift(-1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def process_stock_and_index_data(stock_code, index_code):\n",
    "    # Her iki hisse senedi için veri çekme\n",
    "    stock_df = get_stock_price_history(stock_code)\n",
    "    index_df = get_stock_price_history(index_code)\n",
    "\n",
    "    # RSI ve ATR hesaplamaları yapma\n",
    "    stock_df_with_rsi_and_atr = calculate_atr(calculate_rsi(stock_df))\n",
    "\n",
    "    # Gereksiz sütunları silme\n",
    "    stock_df_cleaned = delete_high_low_columns(stock_df_with_rsi_and_atr)\n",
    "    index_df_cleaned = delete_high_low_columns(index_df)\n",
    "\n",
    "    # Birleştirme ve fiyat değişiklik hesaplamaları\n",
    "    # threshold_values = stock_df_cleaned['ATR'] * 0.5\n",
    "    merged_df = merge_and_calculate_price_change(stock_df_cleaned, index_df_cleaned)\n",
    "\n",
    "    # İlgili sütunları bir satır aşağı kaydırma ve NaN değerlerini silme\n",
    "    merged_df = shift_columns_up(merged_df, ['Relative_change'])\n",
    "    merged_df = delete_nan_values(merged_df)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "proccesed_asels = process_stock_and_index_data('ASELS.IS', 'XUTEK.IS')\n",
    "proccesed_logo = process_stock_and_index_data('LOGO.IS', 'XUTEK.IS')\n",
    "proccesed_turkcell = process_stock_and_index_data('TCELL.IS', 'XUTEK.IS')\n",
    "\n",
    "print(proccesed_asels)\n",
    "print(proccesed_logo)\n",
    "print(proccesed_turkcell)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATING MERGED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bJXzhl_i_tcP",
    "outputId": "0f1354f3-68be-4f2b-a7a3-cba11a5f57da"
   },
   "outputs": [],
   "source": [
    "# Google Drive ile bağlantı kurmak için gerekli kütüphane\n",
    "\n",
    "# !pip install pandas vaderSentiment\n",
    "\n",
    "# from google.colab import drive\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Google Drive'ı bağlama\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "def get_news_history(stock_code):\n",
    "  stock_file_path = f'Data/{stock_code}.xlsx' \n",
    "  selected_columns = ['Date', 'Time', 'Text']\n",
    "  df = pd.read_excel(stock_file_path, usecols=selected_columns)\n",
    "  df = delete_nan_values(df)\n",
    "  df['Date'] = pd.to_datetime(df['Date'], dayfirst=True)\n",
    "  df['Text'] = df['Text'].astype(str)\n",
    "  df_reversed = df.iloc[::-1].reset_index(drop=True) #reverse\n",
    "  return df_reversed\n",
    "\n",
    "def calculate_daily_vader(df, column):\n",
    "    # VADER duygu analizörünü başlat\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # Her bir metin için VADER skorlarını hesapla ve skorları DataFrame'e ekleyerek devam et\n",
    "    df['scores'] = df[column].apply(analyzer.polarity_scores)\n",
    "\n",
    "    # Aynı gün içindeki haberlerin duygu skorlarını ortalama almak için gruplama yap\n",
    "    scores_df = pd.DataFrame(list(df['scores']))\n",
    "    scores_df['Date'] = df['Date']\n",
    "    grouped_scores = scores_df.groupby('Date').mean()\n",
    "\n",
    "    # Ortalama duygu skorlarını ana DataFrame ile birleştir\n",
    "    df = df.drop(columns=['scores']).merge(grouped_scores, on='Date', how='left')\n",
    "\n",
    "    return df\n",
    "\n",
    "def calculate_row_vader(df, column):\n",
    "    # VADER duygu analizörünü başlat\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # Her bir metin için VADER skorlarını hesapla\n",
    "    df['scores'] = df[column].apply(analyzer.polarity_scores)\n",
    "\n",
    "    # Elde edilen duygu skorlarını ayrı sütunlara ayırma\n",
    "    df['Positive'] = df['scores'].apply(lambda x: x['pos'])\n",
    "    df['Negative'] = df['scores'].apply(lambda x: x['neg'])\n",
    "    df['Neutral'] = df['scores'].apply(lambda x: x['neu'])\n",
    "    df['Compound'] = df['scores'].apply(lambda x: x['compound'])\n",
    "\n",
    "    # 'scores' sütununu DataFrame'den çıkar\n",
    "    df.drop(columns=['scores'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def merge_price_and_news(df_prices, df_news):\n",
    "  merged_df = pd.merge_asof(df_prices, df_news, on='Date', direction='backward')\n",
    "  return merged_df\n",
    "\n",
    "\n",
    "def prepare_data_for_model(stock_code, index_code):\n",
    "  proccesed_data = process_stock_and_index_data(stock_code, index_code)\n",
    "\n",
    "  df = get_news_history(f'{stock_code}_kap')\n",
    "  df_vader = calculate_daily_vader(df, 'Text')\n",
    "\n",
    "\n",
    "  df_merged = merge_price_and_news(proccesed_data, df_vader)\n",
    "\n",
    "  df_merged = delete_nan_values_with_column(df_merged, 'Text')\n",
    "\n",
    "  df_merged.to_excel(f'merged_data_{stock_code}.xlsx', index=False)\n",
    "\n",
    "  return df_merged\n",
    "\n",
    "df_asels = prepare_data_for_model('ASELS.IS', 'XUTEK.IS')\n",
    "df_logo = prepare_data_for_model('LOGO.IS', 'XUTEK.IS')\n",
    "df_turkcell = prepare_data_for_model('TCELL.IS', 'XUTEK.IS')\n",
    "\n",
    "print(df_logo)\n",
    "print(df_asels)\n",
    "print(df_turkcell)\n",
    "# 'Date' sütunlarını datetime veri tipine dönüştürme\n",
    "# df_prices['Date'] = pd.to_datetime(df_prices['Date'])\n",
    "# df_news['Date'] = pd.to_datetime(df_news['Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOM FOREST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ebtTHz6twXzt",
    "outputId": "66837bdb-acf3-478f-bbc0-acb16f076693"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "# from google.colab import auth\n",
    "# from oauth2client.client import GoogleCredentials\n",
    "# import gspread\n",
    "from datetime import datetime\n",
    "\n",
    "def train_rf_model_from_excel(stock_code):\n",
    "    stock_file_path = f'Data/merged_data_{stock_code}.xlsx'\n",
    "    selected_columns = ['Date','Change%_stock','Change%_index', 'RSI', 'ATR', 'neg', 'neu', 'pos', 'compound', 'Relative_change']\n",
    "\n",
    "    # Dosyayı okurken başlıkları direkt olarak kullan\n",
    "    df = pd.read_excel(stock_file_path, usecols=selected_columns)\n",
    "\n",
    "    # 'Date' sütununu datetime'a çevirip haftanın gününü çıkar\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['Day_of_Week'] = df['Date'].dt.dayofweek  # Haftanın günü, Pazartesi=0, Pazar=6\n",
    "\n",
    "    # Haftanın günü bilgisini kullanarak modeli eğit\n",
    "    features_kap = df[['RSI', 'ATR','Change%_stock','Change%_index', 'Day_of_Week', 'neg', 'pos']].copy()\n",
    "    features_kap_without_ATR = df[['RSI','Change%_stock','Change%_index', 'Day_of_Week', 'neg', 'pos']].copy()\n",
    "    features = df[['RSI', 'ATR','Change%_stock','Change%_index', 'Day_of_Week']].copy()\n",
    "    target = df['Relative_change'].copy()\n",
    "\n",
    "    # Özellikleri ölçeklendir\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features_kap[['RSI', 'ATR', 'Change%_stock','Change%_index']])\n",
    "    features_kap[['RSI', 'ATR', 'Change%_stock','Change%_index']] = features_scaled\n",
    "\n",
    "    # Veriyi eğitim ve test setlerine ayır\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features_kap, target, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Modeli eğit\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Modelin performansını test et\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1 Score: {f1}')\n",
    "    print('Confusion Matrix:')\n",
    "    print(cm)\n",
    "\n",
    "train_rf_model_from_excel('ASELS.IS')         # 0.46, clean: 0.63\n",
    "#train_rf_model_from_excel('LOGO.IS_clean')    # 0.49, clean: 0.50\n",
    "# train_rf_model_from_excel('TCELL.IS_clean_5')  # 0.45, clean: 0.47_3, 0.43_4, 0.54_5\n",
    "#train_rf_model_from_excel('clean')            # 0.54, ATR'siz: 0.62"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIMIZATION of RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ehjpu4Jwd5MW",
    "outputId": "1b06e1db-2483-44b8-a05d-d5138fb0b14c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def train_model_from_excel(stock_code):\n",
    "    stock_file_path = f'Data/merged_data_{stock_code}.xlsx'\n",
    "    selected_columns = ['Date','Change%_stock','Change%_index', 'RSI', 'ATR', 'neg', 'neu', 'pos', 'compound', 'Relative_change']\n",
    "\n",
    "    df = pd.read_excel(stock_file_path, usecols=selected_columns)\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['Day_of_Week'] = df['Date'].dt.dayofweek  # Haftanın günü, Pazartesi=0, Pazar=6\n",
    "\n",
    "    features_kap = df[['RSI', 'ATR','Change%_stock','Change%_index', 'Day_of_Week', 'neg', 'pos']].copy()\n",
    "    target = df['Relative_change'].copy()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features_kap[['ATR', 'RSI','Change%_stock','Change%_index']])\n",
    "    features_kap[['ATR', 'RSI','Change%_stock','Change%_index']] = features_scaled\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features_kap, target, test_size=0.3, random_state=42)\n",
    "\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, 30, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X_resampled, y_resampled)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(f'Best parameters found: {grid_search.best_params_}')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1 Score: {f1}')\n",
    "    print('Confusion Matrix:')\n",
    "    print(cm)\n",
    "\n",
    "train_model_from_excel('ASELS.IS')\n",
    "#train_model_from_excel('LOGO.IS_clean')\n",
    "# train_model_from_excel('TCELL.IS_clean_5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting Machines (GBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting Machines (GBM)\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "# from google.colab import auth\n",
    "# from oauth2client.client import GoogleCredentials\n",
    "# import gspread\n",
    "from datetime import datetime\n",
    "\n",
    "def train_gradient_boosting_model_from_excel(stock_code):\n",
    "    stock_file_path = f'/Data/merged_data_{stock_code}.xlsx'\n",
    "    # stock_file_path = f'/content/drive/My Drive/Data/merged_data_{stock_code}.xlsx'\n",
    "    # stock_file_path = f'/content/drive/My Drive/Data/{stock_code}.xlsx'\n",
    "    selected_columns = ['Date','Change%_stock','Change%_index', 'RSI', 'ATR', 'neg', 'neu', 'pos', 'compound', 'Relative_change']\n",
    "\n",
    "    # Dosyayı okurken başlıkları direkt olarak kullan\n",
    "    df = pd.read_excel(stock_file_path, usecols=selected_columns)\n",
    "\n",
    "    # 'Date' sütununu datetime'a çevirip haftanın gününü çıkar\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['Day_of_Week'] = df['Date'].dt.dayofweek  # Haftanın günü, Pazartesi=0, Pazar=6\n",
    "\n",
    "    # Haftanın günü bilgisini kullanarak modeli eğit\n",
    "    features_kap = df[['RSI', 'ATR','Change%_stock','Change%_index', 'Day_of_Week', 'neg', 'pos']].copy()\n",
    "    features_kap_without_ATR = df[['RSI','Change%_stock','Change%_index', 'Day_of_Week', 'neg', 'pos']].copy()\n",
    "    features = df[['RSI', 'ATR','Change%_stock','Change%_index', 'Day_of_Week']].copy()\n",
    "    target = df['Relative_change'].copy()\n",
    "\n",
    "    # Özellikleri ölçeklendir\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features_kap[['RSI', 'Change%_stock','Change%_index']])\n",
    "    features_kap[['RSI', 'Change%_stock','Change%_index']] = features_scaled\n",
    "\n",
    "    # Veriyi eğitim ve test setlerine ayır\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features_kap, target, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Modeli eğit\n",
    "    model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Modelin performansını test et\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1 Score: {f1}')\n",
    "    print('Confusion Matrix:')\n",
    "    print(cm)\n",
    "\n",
    "train_gradient_boosting_model_from_excel('ASELS.IS')         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "LSTM \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kkKZrlpUz5VO",
    "outputId": "c8c0e7ff-511d-4474-d02e-ae1a6aa13899"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from datetime import datetime\n",
    "\n",
    "def create_model(lstm_units=50, dropout_rate=0.2, num_classes=3):\n",
    "    model = Sequential([\n",
    "        LSTM(lstm_units, input_shape=(1, 5), return_sequences=True),  # Girdi şeklini 5 özellik olarak güncelledim\n",
    "        Dropout(dropout_rate),\n",
    "        LSTM(lstm_units, return_sequences=False),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_lstm_model(stock_code):\n",
    "    stock_file_path = f'Data/merged_data_{stock_code}.xlsx'\n",
    "    selected_columns = ['Date', 'Change%_stock', 'Change%_index', 'RSI', 'ATR', 'Relative_change']\n",
    "\n",
    "    df = pd.read_excel(stock_file_path, usecols=selected_columns)\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['Day_of_Week'] = df['Date'].dt.dayofweek\n",
    "\n",
    "    features = df[['RSI', 'ATR', 'Change%_stock', 'Change%_index', 'Day_of_Week']].copy()\n",
    "    target = df['Relative_change'].copy()\n",
    "\n",
    "    # Encode the target variable\n",
    "    encoder = LabelEncoder()\n",
    "    target_encoded = encoder.fit_transform(target)\n",
    "    num_classes = len(np.unique(target_encoded))  # Dinamik olarak sınıf sayısını belirle\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "    # Convert target to categorical\n",
    "    target_encoded = to_categorical(target_encoded)\n",
    "\n",
    "    # Reshape input to be [samples, time steps, features]\n",
    "    features_scaled = np.reshape(features_scaled, (features_scaled.shape[0], 1, features_scaled.shape[1]))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features_scaled, target_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Create and train the model\n",
    "    model = create_model(lstm_units=50, dropout_rate=0.2, num_classes=num_classes)\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "    # Evaluate the model\n",
    "    _, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f'LSTM Model Accuracy: {accuracy:.2f}')\n",
    "\n",
    "#train_lstm_model('ASELS.IS_clean')\n",
    "#train_model_from_excel('LOGO.IS_clean')\n",
    "train_model_from_excel('TCELL.IS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USER INTERFACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JxUI3eUXwc7M",
    "outputId": "a3010077-b409-4bb3-f4f1-15412a121200"
   },
   "outputs": [],
   "source": [
    "# !pip install imbalanced-learn\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, messagebox\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def train_model_from_excel(stock_code):\n",
    "    stock_file_path = f'tezData/merged_data_{stock_code}_clean.xlsx'\n",
    "    selected_columns = ['Date', 'Change%_stock', 'Change%_index', 'RSI', 'ATR', 'neg', 'neu', 'pos', 'compound', 'Relative_change']\n",
    "\n",
    "    df = pd.read_excel(stock_file_path, usecols=selected_columns)\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['Day_of_Week'] = df['Date'].dt.dayofweek\n",
    "\n",
    "    features_kap = df[['RSI', 'ATR', 'Change%_stock', 'Change%_index', 'Day_of_Week', 'neg', 'pos']].copy()\n",
    "    target = df['Relative_change'].copy()\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features_kap[['ATR', 'RSI', 'Change%_stock', 'Change%_index']])\n",
    "    features_kap[['ATR', 'RSI', 'Change%_stock', 'Change%_index']] = features_scaled\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features_kap, target, test_size=0.3, random_state=42)\n",
    "\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 20, 30, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X_resampled, y_resampled)\n",
    "\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return accuracy, precision, recall, f1, cm, grid_search.best_params_\n",
    "\n",
    "# Tkinter GUI oluşturma\n",
    "root = tk.Tk()\n",
    "root.title(\"Stock Performance Analysis\")\n",
    "\n",
    "# Şirketler ve modeller listesi\n",
    "companies = ['ASELS.IS', 'LOGO.IS', 'TCELL.IS']\n",
    "models = ['Random Forest']\n",
    "\n",
    "selected_company = tk.StringVar()\n",
    "selected_model = tk.StringVar()\n",
    "\n",
    "# Şirket seçimi\n",
    "company_label = ttk.Label(root, text=\"Select Company:\")\n",
    "company_label.pack(pady=5)\n",
    "company_combo = ttk.Combobox(root, textvariable=selected_company, values=companies)\n",
    "company_combo.pack(pady=5)\n",
    "\n",
    "# Model seçimi\n",
    "model_label = ttk.Label(root, text=\"Select Model:\")\n",
    "model_label.pack(pady=5)\n",
    "model_combo = ttk.Combobox(root, textvariable=selected_model, values=models)\n",
    "model_combo.pack(pady=5)\n",
    "\n",
    "def on_submit():\n",
    "    company = selected_company.get()\n",
    "    model_type = selected_model.get()\n",
    "    \n",
    "    if model_type == 'Random Forest':\n",
    "        accuracy, precision, recall, f1, cm, best_params = train_model_from_excel(company)\n",
    "        result_text = f\"\"\"\n",
    "        Model: {model_type}, Company: {company}\n",
    "        Accuracy: {accuracy:.2f}\n",
    "        Precision: {precision:.2f}\n",
    "        Recall: {recall:.2f}\n",
    "        F1 Score: {f1:.2f}\n",
    "        Confusion Matrix: {cm}\n",
    "        Best Parameters: {best_params}\n",
    "        \"\"\"\n",
    "        messagebox.showinfo(\"Model Results\", result_text)\n",
    "\n",
    "submit_button = ttk.Button(root, text=\"Submit\", command=on_submit)\n",
    "submit_button.pack(pady=20)\n",
    "\n",
    "root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
